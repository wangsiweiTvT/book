---
layout: post
title: 偏见和毒舌
---

偏见和毒舌，是我国社会的毒瘤。所谓偏见，我理解就是给人贴标签。我看到，所有的人其实都有这个倾向，也受到它的困扰，比如，给一个人贴上“公知”的标签。而毒舌，特别能够激发旁观者的情绪，获得关注和流量，因此在我们的社会中非常盛行。而“理中客”，如今已成为一个人人避之不及的负面标签，由此可见一斑。

在我们的系统中，偏见和毒舌，可能是一个“诊断”的对象。这应该是一个附加的功能，底线。

## 课程材料

- 斯坦福 CS324 2022 年 [Harm 1](https://stanford-cs324.github.io/winter2022/lectures/harms-1/)， [Harm 2](https://stanford-cs324.github.io/winter2022/lectures/harms-2/)

- 约翰霍普金斯 UA 2024 Lec 23 Social concerns about LMs
  - Bias, fairness and toxic language
  - Hallucination, truthfulness, and veracity,

Suggested Reading: 
- Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models
Additional Reading:
- UnQovering Stereotyping Biases via Underspecified Questions
- Robots Enact Malignant Stereotypes
- Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias
- Red Teaming Language Models with Language Models
- RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models
- TruthfulQA: Measuring How Models Mimic Human Falsehoods
- Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus

- 约翰霍普金斯 UA 2024 Lec 24 Legal considerations and fair use
    - Future dangers and misuses,
    - Reflections about future,

Additional Reading:
- Foundation Models and Fair Use
- Copyright and the Generative-AI Supply Chain

## 练习

- 斯坦福 CS324 2022 年 Project 1 Evaluating LLM Task 2， [PDF](https://stanford-cs324.github.io/winter2022/projects/CS324_P1.pdf)，评估 GPT-3 的 Bias

## 论文

JHU 课程推荐论文

Bias
- Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models

Additional Reading:
- UnQovering Stereotyping Biases via Underspecified Questions
- Robots Enact Malignant Stereotypes
- Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias
- Red Teaming Language Models with Language Models
- RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models
- TruthfulQA: Measuring How Models Mimic Human Falsehoods
- Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus
- CommunityLM: Probing Partisan Worldviews from Language Models.
- Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP

Toxicity
- RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models

Additional Reading(s):
- On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?
- TruthfulQA: Measuring How Models Mimic Human Falsehoods
- Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus

Fair Use
- Foundation Models and Fair Use
- Copyright and the Generative-AI Supply Chain

下面是普林斯顿大学课程的推荐论文。

### evaluation 
- [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/pdf/2009.11462.pdf)
- [OPT paper, Section 4](https://arxiv.org/pdf/2205.01068.pdf)

Refer:
- [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)
- [Red Teaming Language Models with Language Models](https://storage.googleapis.com/deepmind-media/Red%20Teaming/Red%20Teaming.pdf)
- [Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection](https://arxiv.org/pdf/2201.10474.pdf)

### Mitigation
- [Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP](https://arxiv.org/pdf/2103.00453.pdf)

Refer:
- [Challenges in Detoxifying Language Models](https://arxiv.org/pdf/2109.07445.pdf)
- [Detoxifying Language Models Risks Marginalizing Minority Voices](https://arxiv.org/pdf/2104.06390.pdf)
- [Plug and Play Language Models: A Simple Approach to Controlled Text Generation](https://arxiv.org/pdf/1912.02164.pdf)
- [GeDi: Generative discriminator guided sequence generation](https://arxiv.org/pdf/2009.06367.pdf)

## Demo

- [Social stereotypes in models](https://unqover.apps.allenai.org/)

<br/>

| [Index](./) | [Previous](7-5-moral) | [Next](7-9-security)
