---
layout: post
title: Policy Gradient
---

用梯度上升的方法，根据实验结果，自动优化神经元网络，获得优化的策略，这就是策略梯度。

我们下面学习策略梯度（Policy Gradient）方法。它直接学习策略模型。通过 Log 技巧，它以测量获得的 reward 为权重，对策略模型的输出 Action 概率 Log 后对模型参数的导数进行加权，这样就能增强能够让 Reward 增加的梯度。这就是“增强学习”的含义。REINFORCE 就是这样的一种算法。AlphaGo 也是基于这种方法。

我们前面学过 Q-learning。除了 Q-learning，常用的 Model-free online RL 方法还有 Policy Gradient 和 Actor Critic。其中，Policy Gradient 是基于 Policy 的方法，Actor Critic 则是即基于 Policy 又基于 Value。我们下面学习 Policy Gradient。

## Policy Gradient 的学习目标

我们首先学习 Policy Gradient。我们通过比较 Policy Gradient 和 Q-learning，来解释 PG 算法的特点：

首先，介绍它们的两个相同点：

第一，和 Q-learning 一样，它是一种 Model-Free 算法。这就是说，它并不去学习 MDP 的 Transition、Reward，而是通过大量的实验，直接学习如何决策。Q-learning 也是这样一种方法：它不需要清楚地计算 Transition 和 Reward 模型，而是直接学习要做决策需要的信息。

第二，和 Q-learning 一样，它也是一种 Online 更新的模型，即在实验的过程中，就更新模型。

然后，我们介绍它们的不同点：Policy Gradient 和 Q-learning 学习的“东西”不一样。

具体来说，Q-learning 会学习 Q(s,a) 的“值”（Value），然后通过选择让这个值最大的 a，作为最佳策略。所以我们说 Q-learning 是一种基于 Value 的 Model-free online RL 方法。

那么，Policy Gradient 学习什么呢？它直接学习 Policy。正如它的名字 Policy Gradient 说明的，它的模型输入是当前系统的状态 s，输出是要执行的下一步动作 Action a；所以，它学习的就是 Policy：p(a|s)。它不需要“值函数”了。

## 有监督的策略学习方法

如果我们有很多专家的策略数据，那么我们当然可以训练一个有监督的模型，来学习 p(a|s)。此时，我们的深度模型的输出是 Softmax 后的各个 Action 的概率。我们可以把优化目标设置为“最大化专家策略数据的 Log 似然”，然后采用梯度上升，来优化。

此时的模型非常好实现，就是经典的深度学习模型。其目标函数，就是以专家 Action 为目标 Label，以模型输出的 Softmax 为模型结果，计算两者的交叉熵。这就是专家 Action 的似然。然后，我们的目标是最大化该目标函数，因此，就是最大似然。具体代码，请参考伯克利 CS182 的 PPT。

## 增强学习的策略学习方法

当我们不依靠专家的策略数据，而是依靠“自己的实践”来学习策略，这就是“增强学习”。这时，我们会通过实验，来获得实验数据。在一次序列实验中，我们会获得很多 (state, action, reward) 的三元组序列。而模型的目标函数，是整个样本序列“折扣后的 Reward 和”。我们希望它最大。

那么，我们怎么基于测量的三元组序列，来获得我们的目标函数对模型参数的梯度呢？对这一点，请参考斯坦福大学 CS224n 大语言模型的 PPT。它介绍 RLHF 算法时对这个方法介绍得最清楚。下面是简介：

首先，我们的目标函数是 Reward 的期望，而我们要求它对策略模型参数的梯度。这里的困难在于我们的 Reward 并不是“可微”的（因为它是我们测量的结果，不是模型生成的）。那么，怎么求它对策略模型参数的梯度呢？神奇的 Williams 老师在 1992 年想出了办法，即 REINFORCE 算法。

REINFORCE 算法用了一个 Log 技巧。具体来说，我们本来要求的是“样本序列”的 Reward 的期望对模型参数的导数。我们通过数学变换，可以把我们的目标函数对模型参数的梯度改写为：以测量获得的 reward 为权重，对策略模型的输出 Action 概率 Log 后对模型参数的导数，进行加权。这个结果，就是我们想要的。

基于 REINFORCE 算法，我们就可以基于实验的样本序列，得到上述梯度了。因为实验的过程就是蒙特卡洛取样的过程，所以这也被叫做“蒙特卡洛策略梯度”方法。

这也正是“增强学习”为什么叫做“增强”的原因。因为用 Reward 做加权，这意味着，如果 Reward > 0，那么我们就会加上这个梯度，那么模型的参数就会往这个方向调，相对于“增强”这个方向；相反，如果 Reward < 0，那么我们就会减去这个梯度，那么模型的参数就会往反方向调。这就是我们为什么叫这个算法是“增强学习”的原因。利用这种方法，对任意不可微的 reward 函数，我们就都可以做增强学习了。这一部分内容，详见斯坦福大学 CS224n PPT。

为了让 PG 算法在实际上可行，我们引入一个技巧：“Reward to go”。因为一个 Action 之前的 Reward 不应该被用来加权这个 Action 的梯度，所以，我们会对每个 Action 对梯度，仅乘该 Action 之后的 Reward 和。这能够减少一个 Action 之前的 Reward 的影响，让模型更快收敛。用术语说，就是减少模型的 Variance。在机器学习中，通常把这种只允许“发生时间在前的变量”影响“发生时间在后的变量”的限制，叫做“因果（Causality）”。因此，我们也常叫这个技巧为“考虑因果”。对这个技巧，请参考伯克利 CS182 的 PPT 和课堂讨论（Discussion）材料。

我们看最后的 REINFORCE 算法，非常干净，漂亮。它就是做一连串实验（学术上说，就是做蒙特卡洛采样）后，统计每一步的“后续”衰减累积 Reward，然后用它作为权重，加权这一步时的模型对这一步 Action 的预测概率的 Log，然后再乘一个 n 阶的衰减，作为梯度上升的梯度。最后乘的这个 n 阶的衰减，是因为序列模型导致的。详见滑铁卢大学 PPT。

注意，它统计的、每一步的“后续”衰减累积 Reward 常被记作 $$G_n$$。我们后续会基于它进行各种改进。

## 策略梯度在实际中的问题

策略梯度在实际中有两个难题：

1）High Variance：虽然 REINFORCE 能够通过实验获得目标函数的梯度了，但因为实验中的影响因素众多，实验获得的梯度的分布范围很广，有很大的噪声，因此它是 High Variance 的。我们知道，对付 High Varaiance 的问题，可以通过扩大数据规模（Size）来缓解，因此，在训练 PG 时，我们一般会用很大的 Batch Size。

2）对 Learning Rate 这个参数的调节很难。ADAM 这些自适应步长设置方法工作一般。

因此，在实际中，PG 算法是很需要大量实践，积累经验的。

## 部分可观察（POMDP）

PG 算法的一个非常重要的特性是：它并没有用到状态的马尔可夫性。因此，它可以被用于 POMDP。这在实际中很有用。所以它是一个非常通用的增强学习算法。

## 应用

策略梯度在实际中运用很广。我们看三个例子：

第一个例子是 NIPS 2014 的《Recurrent Models of Visual Attention》工作。其中，研究者用 PG 来获得最佳的视觉 Attention 的“视野”。

首先看它的模型：它将“视野”的优化问题，模型为一个部分可观测的马尔科夫决策过程 POMDP。这个视野，即Hard Attention。它由一个 RNN 输出。RNN 的输入是 t-1 时刻隐状态 + 当前时刻的图片内容及当前“视野”经过两个全连接网络的编码结果。因此，状态 s 和 RNN 的隐状态 h 有关，是不可观测的。

然后看它的 Action 和 Reward。该模型的 Action 有两个。每次决策会产生两个 Action：视野中心的位置 L 和图片分类的动作（一个 Softmax 动作）。它的Reward 是在一个 Episode 结束后，图片分类正确为 1，错误是 0。一个 Episode 就是给一张图片，然后“视野”从初始位置开始，到最后一个时间步长，得到图片分类结果结束。

实验结果特别有意思：我们会看到 RNN 会慢慢的把“视野”移到正确的地方，正好把要识别的数字框住。我们可以看到视野的移动过程。

这个例子给我们很大的启发，就是说：只要 Reward 设定好，就可以通过 Reward 的加权提供的信号，控制深度学习模型，慢慢找到最优的结果。

第二个例子是用它来控制机器人。

第三个例子是用它来调节离散 Latent 变量模型的模型参数。

上面这部分内容，请参考伯克利 CS182 的 PPT。

## AlphaGo

AlphaGo 2017 年 5 月，打败柯洁。柯洁说：“去年，AlphaGo 下棋时仍然很像人类。但今年，它变得像围棋之神一样。”

AlphaGo 采取了四个方法。

第一个方法，就是我们刚刚学过的“有监督策略学习”：它首先学习 30M 的围棋棋谱，模仿专家的策略，这是有监督学习；

第二个方法，是“策略网络”。这就是我们刚刚学过的“增强学习的策略梯度方法”。它让程序和自己以前的版本比赛，在这个过程中，学习最优策略。具体来说，就是根据最后的输赢，对这轮比赛中每一步的 Log 策略梯度，用折扣后的 Reward 加权：赢的话，+1，输的话，-1。然后做梯度上升，调整模型参数。

第三个方法，是“值网络”；用一个神经元网络，在每个状态，预测谁会赢，即衰减后Reward 的和的期望 V(s)。它用梯度下降来优化这个神经元网络，提高它的预测准确度。

第四个方法，是基于上面的 Policy 和 Value 网络的结果，做蒙特卡洛树搜索。我们后面会学到它。

## 学习材料

请继续学习以下材料：

## 课程材料

- 滑铁卢大学 CS486 人工智能 PG slides
- 伯克利 CS182 深度学习，Policy Gradient PPT，[B 站视频](https://www.bilibili.com/video/BV1PK4y1U751?p=45)，讨论材料 9
- Berkeley CS285 
  - Lec 5: Policy Gradients, [slides](https://rail.eecs.berkeley.edu/deeprlcourse/), [Youtube Video](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps)
  - Lec 9: Advanced Policy Gradients, [slides](https://rail.eecs.berkeley.edu/deeprlcourse/), [Youtube Video](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps)
- Stanford CS234 RL Lecture 7/8: Policy search
- Silver RL 2015 Lec 7: Policy Gradient Methods [website](https://www.davidsilver.uk/teaching/), [video](https://www.youtube.com/watch?v=2pWv7GOvuf0)
- DeepMind UCL Hadovan RL 2021 Lec 9 Policy gradients and actor critics

## 课本材料

- [SutBar] Sec. 13.1-13.3, 13.7 [SigBuf] Sec. 5.1-5.2, [RusNor] Sec. 21.5

## 练习

- ms-DeepRL
- Berkeley Deep RL Bootcamp 2017, Lecture 4b, Pong from Pixels -- Andrej Karpathy  ([video](https://www.youtube.com/watch?v=tqrcjHuNdmQ)，[slides](https://drive.google.com/file/d/0BxXI_RttTZAhTUpqUFdEZ3BXNFE/view?usp=sharing&resourcekey=0-umYd645mR9LiaKZdjvVH-g)), [Website](https://sites.google.com/view/deep-rl-bootcamp/lectures), [Source code](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)
- 伯克利 CS285 HW 2: Policy gradients, [Website](https://rail.eecs.berkeley.edu/deeprlcourse/)
- 伯克利 CS182 DL MuJoCo implement Imitation learning, Policy Gradients, DQN, and Actor Critic algorithms.
- 伯克利 RL bootcamp 2017 
  - Karpathy PG 
  - Lab 4 PG
- Denny Britz + Silver RL 2015 Lab 7

## 论文

斯坦福 CS 224r 课程提到的论文

MDPs and Policy Gradients
- Simple statistical gradient-following algorithms for connectionist reinforcement learning. Williams (1992)

伯克利 论文

Classic papers
- Williams (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning: introduces REINFORCE algorithm
- Baxter & Bartlett (2001). Infinite-horizon policy-gradient estimation: temporally decomposed policy gradient (not the first paper on this! see actor-critic section later)
- Peters & Schaal (2008). Reinforcement learning of motor skills with policy gradients: very accessible overview of optimal baselines and natural gradient

<br/>

|[Index](index) | [Previous](10-imitation) | [Next](13-actor-critic) |
